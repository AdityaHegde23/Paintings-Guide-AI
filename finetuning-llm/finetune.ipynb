{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eeb40527",
   "metadata": {},
   "source": [
    "\n",
    "## Fine-Tuning LLM for Museum Audio Guide Generation\n",
    "## Step-by-step notebook for training a model to generate engaging audio guides for paintings\n",
    "\n",
    "#### Step 4: Prepare Training Data\n",
    "##### We'll convert our JSON training examples into the proper format for fine-tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74a11488",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ad/Paintings-Guide-AI/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41b4f7b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loading training data...\n",
      " Loaded 938 training examples\n"
     ]
    }
   ],
   "source": [
    "print(\" Loading training data...\")\n",
    "with open('raw-data/training_examples_bulk.json', 'r', encoding='utf-8') as f:\n",
    "    training_data = json.load(f)\n",
    "\n",
    "print(f\" Loaded {len(training_data)} training examples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac7683bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Sample training example:\n",
      "PROMPT:\n",
      "Title: Self-Portrait with a Straw Hat (obverse: The Potato Peeler)\n",
      "Artist: Vincent van Gogh\n",
      "Date: 1887\n",
      "Medium: Oil on canvas\n",
      "Dimensions: 16 x 12 1/2 in. (40.6 x 31.8 cm)\n",
      "\n",
      "Audio guide:\n",
      "\n",
      "COMPLETION:\n",
      " Meet Van Gogh in his Paris self-portrait from 1887, measuring just 16 by 12 inches. Here, he experiments with Impressionist techniques, using short, vibrant brushstrokes and a lighter palette than his later works. Notice the confident gaze and the casual straw hat—symbols of his artistic evolution. The reverse side reveals The Potato Peeler, showing Van Gogh's resourceful use of canvas during his financially constrained years. <END>\n"
     ]
    }
   ],
   "source": [
    "# Let's examine our data structure\n",
    "print(\"\\n Sample training example:\")\n",
    "sample = training_data[0]\n",
    "print(\"PROMPT:\")\n",
    "print(sample['prompt'])\n",
    "print(\"\\nCOMPLETION:\")  \n",
    "print(sample['completion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47bbd4f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (938, 2)\n",
      "Columns: ['prompt', 'completion']\n",
      "Data quality check:\n",
      "- Examples with empty prompts: 0\n",
      "- Examples with empty completions: 0\n",
      "- Average prompt length: 229 characters\n",
      "- Average completion length: 387 characters\n"
     ]
    }
   ],
   "source": [
    "# Convert to DataFrame for easier manipulation\n",
    "df = pd.DataFrame(training_data)\n",
    "print(f\"Data shape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "\n",
    "# Check for any issues in the data\n",
    "print(\"Data quality check:\")\n",
    "print(f\"- Examples with empty prompts: {df['prompt'].isin(['', None]).sum()}\")\n",
    "print(f\"- Examples with empty completions: {df['completion'].isin(['', None]).sum()}\")\n",
    "print(f\"- Average prompt length: {df['prompt'].str.len().mean():.0f} characters\")\n",
    "print(f\"- Average completion length: {df['completion'].str.len().mean():.0f} characters\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c859e333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Splitting data into train/validation sets...\n",
      "Training examples: 844\n",
      "Validation examples: 94\n",
      "Saved train/validation splits to processed-data/\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"\\nSplitting data into train/validation sets...\")\n",
    "\n",
    "# 90% train, 10% validation\n",
    "train_data, val_data = train_test_split(\n",
    "    training_data, \n",
    "    test_size=0.1, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training examples: {len(train_data)}\")\n",
    "print(f\"Validation examples: {len(val_data)}\")\n",
    "\n",
    "# Save splits for reference\n",
    "os.makedirs('processed-data', exist_ok=True)\n",
    "\n",
    "with open('processed-data/train_data.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(train_data, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "with open('processed-data/val_data.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(val_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"Saved train/validation splits to processed-data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab92267a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model and tokenizer...\n",
      "Attempting optimized model loading...\n",
      "4-bit quantization config:\n",
      "- Quantization type: NF4 (normalized float 4-bit)\n",
      "- Double quantization: Enabled\n",
      "- Compute dtype: float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: BNB_CUDA_VERSION=121 environment variable detected; loading libbitsandbytes_cuda121.so.\n",
      "This can be used to load a bitsandbytes version built with a CUDA version that is different from the PyTorch CUDA version.\n",
      "If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:17<00:00,  5.80s/it]\n"
     ]
    }
   ],
   "source": [
    "# Load Base Model\n",
    "print(\"Loading base model and tokenizer...\")\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "import torch\n",
    "\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"  # Faster downloads\n",
    "os.environ[\"BITSANDBYTES_NOWELCOME\"] = \"1\"\n",
    "os.environ[\"BNB_CUDA_VERSION\"] = \"121\"\n",
    "\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "\n",
    "print(\"Attempting optimized model loading...\")\n",
    "\n",
    "# Configure 4-bit quantization\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "print(\"4-bit quantization config:\")\n",
    "print(f\"- Quantization type: NF4 (normalized float 4-bit)\")\n",
    "print(f\"- Double quantization: Enabled\") \n",
    "print(f\"- Compute dtype: float16\")\n",
    "\n",
    "# Load model with quantization\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7926ef9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded tokenizer for mistralai/Mistral-7B-Instruct-v0.3\n",
      "Added padding token\n",
      "Vocabulary size: 32768\n",
      "EOS token: '</s>'\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    print(f\"Loaded tokenizer for {MODEL_NAME}\")\n",
    "    \n",
    "    # Add padding token if it doesn't exist\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        print(\"Added padding token\")\n",
    "        \n",
    "    print(f\"Vocabulary size: {tokenizer.vocab_size}\")\n",
    "    print(f\"EOS token: '{tokenizer.eos_token}'\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading tokenizer: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27bade5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Formatting data for training...\n",
      "Formatted 844 training examples\n",
      "Formatted 94 validation examples\n",
      "\n",
      "Created datasets:\n",
      "- Training: 844 examples\n",
      "- Validation: 94 examples\n",
      "\n",
      "Sample formatted text (first 200 chars):\n",
      "'Title: \"Isfandiyar\\'s Third Course: He Slays a Dragon\", Folio 434v from the Shahnama (Book of Kings) of Shah Tahmasp\\nArtist: Abu\\'l Qasim Firdausi\\nDate: ca. 1530\\nMedium: Opaque watercolor, ink, silver, '\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Format Data for Training\n",
    "# Convert our prompt/completion format to the format expected by the model\n",
    "\n",
    "def format_training_example(example):\n",
    "    \"\"\"Format a single training example for causal language modeling\"\"\"\n",
    "    \n",
    "    # Combine prompt and completion into single text\n",
    "    # This is the format for causal LM training\n",
    "    full_text = example['prompt'] + example['completion']\n",
    "    \n",
    "    return {\n",
    "        'text': full_text,\n",
    "        'prompt': example['prompt'],\n",
    "        'completion': example['completion']\n",
    "    }\n",
    "\n",
    "print(\"\\nFormatting data for training...\")\n",
    "\n",
    "# Format training and validation data\n",
    "formatted_train = [format_training_example(ex) for ex in train_data]\n",
    "formatted_val = [format_training_example(ex) for ex in val_data]\n",
    "\n",
    "print(f\"Formatted {len(formatted_train)} training examples\")\n",
    "print(f\"Formatted {len(formatted_val)} validation examples\")\n",
    "\n",
    "# Create Hugging Face datasets\n",
    "train_dataset = Dataset.from_list(formatted_train)\n",
    "val_dataset = Dataset.from_list(formatted_val)\n",
    "\n",
    "print(f\"\\nCreated datasets:\")\n",
    "print(f\"- Training: {len(train_dataset)} examples\")\n",
    "print(f\"- Validation: {len(val_dataset)} examples\")\n",
    "\n",
    "# Show a formatted example\n",
    "print(f\"\\nSample formatted text (first 200 chars):\")\n",
    "print(repr(formatted_train[0]['text'][:200]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16bef8c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenizing datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 844/844 [00:00<00:00, 2954.44 examples/s]\n",
      "Map: 100%|██████████| 94/94 [00:00<00:00, 4110.31 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized training data: 844 examples\n",
      "Tokenized validation data: 94 examples\n",
      "\n",
      " Tokenization verification:\n",
      "Input length: 512\n",
      "Label length: 512\n",
      "Masked tokens (prompt): 198\n",
      "Training tokens (completion): 314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Tokenize the Data (Fixed Version)\n",
    "# Convert text to tokens that the model can understand\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"Tokenize the text data with proper prompt masking\"\"\"\n",
    "    \n",
    "    # Tokenize the full text (prompt + completion)\n",
    "    tokenized = tokenizer(\n",
    "        examples['text'],\n",
    "        truncation=True,\n",
    "        padding='max_length',  \n",
    "        max_length=512,\n",
    "        return_tensors=None\n",
    "    )\n",
    "    \n",
    "    # Tokenize just the prompts (to find where completion starts)\n",
    "    prompt_tokenized = tokenizer(\n",
    "        examples['prompt'],\n",
    "        truncation=True,\n",
    "        padding=False,  # Don't pad prompts\n",
    "        max_length=512,\n",
    "        add_special_tokens=False,  # Don't add extra tokens\n",
    "        return_tensors=None\n",
    "    )\n",
    "    \n",
    "    # Create labels with prompt masking\n",
    "    labels = []\n",
    "    for i in range(len(tokenized['input_ids'])):\n",
    "        # Get the full sequence and prompt length\n",
    "        full_ids = tokenized['input_ids'][i]\n",
    "        prompt_length = len(prompt_tokenized['input_ids'][i])\n",
    "        \n",
    "        # Create label sequence\n",
    "        label_ids = full_ids.copy()\n",
    "        \n",
    "        # Mask prompt tokens (set to -100 so they're ignored in loss)\n",
    "        for j in range(min(prompt_length, len(label_ids))):\n",
    "            label_ids[j] = -100\n",
    "        \n",
    "        labels.append(label_ids)\n",
    "    \n",
    "    tokenized['labels'] = labels\n",
    "    return tokenized\n",
    "\n",
    "print(\"\\nTokenizing datasets...\")\n",
    "\n",
    "# Tokenize in batches for efficiency\n",
    "tokenized_train = train_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names\n",
    ")\n",
    "\n",
    "tokenized_val = val_dataset.map(\n",
    "    tokenize_function, \n",
    "    batched=True,\n",
    "    remove_columns=val_dataset.column_names\n",
    ")\n",
    "\n",
    "print(f\"Tokenized training data: {len(tokenized_train)} examples\")\n",
    "print(f\"Tokenized validation data: {len(tokenized_val)} examples\")\n",
    "\n",
    "# Verify tokenization worked\n",
    "print(\"\\n Tokenization verification:\")\n",
    "sample = tokenized_train[0]\n",
    "print(f\"Input length: {len(sample['input_ids'])}\")\n",
    "print(f\"Label length: {len(sample['labels'])}\")\n",
    "print(f\"Masked tokens (prompt): {sum(1 for x in sample['labels'] if x == -100)}\")\n",
    "print(f\"Training tokens (completion): {sum(1 for x in sample['labels'] if x != -100 and x != tokenizer.pad_token_id)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31ebca70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configuring LoRA (Low-Rank Adaptation)...\n",
      "LoRA Configuration:\n",
      "- Rank (r): 16\n",
      "- Alpha: 32\n",
      "- Dropout: 0.1\n",
      "- Target modules: {'o_proj', 'k_proj', 'gate_proj', 'up_proj', 'down_proj', 'q_proj', 'v_proj'}\n",
      "\n",
      "Applying LoRA adapters to model...\n",
      "Parameter Statistics:\n",
      "- Trainable parameters: 41,943,040\n",
      "- Total parameters: 3,800,305,664\n",
      "- Trainable percentage: 1.10%\n",
      "VRAM after LoRA: 4.51 GB\n"
     ]
    }
   ],
   "source": [
    "# Step 9: Configure PEFT/LoRA for Efficient Fine-tuning\n",
    "# LoRA (Low-Rank Adaptation) allows us to fine-tune efficiently by only training small adapter layers\n",
    "\n",
    "from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training\n",
    "import torch\n",
    "\n",
    "print(\"\\nConfiguring LoRA (Low-Rank Adaptation)...\")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "\n",
    "# LoRA Configuration  \n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"LoRA Configuration:\")\n",
    "print(f\"- Rank (r): {lora_config.r}\")\n",
    "print(f\"- Alpha: {lora_config.lora_alpha}\")\n",
    "print(f\"- Dropout: {lora_config.lora_dropout}\")\n",
    "print(f\"- Target modules: {lora_config.target_modules}\")\n",
    "\n",
    "# Apply LoRA to the model\n",
    "print(\"\\nApplying LoRA adapters to model...\")\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Enable gradient computation for LoRA parameters\n",
    "model.enable_input_require_grads()\n",
    "\n",
    "# Check trainable parameters\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "all_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(f\"Parameter Statistics:\")\n",
    "print(f\"- Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"- Total parameters: {all_params:,}\")\n",
    "print(f\"- Trainable percentage: {100 * trainable_params / all_params:.2f}%\")\n",
    "\n",
    "# Check memory usage after LoRA\n",
    "if torch.cuda.is_available():\n",
    "    vram_used = torch.cuda.memory_allocated() / 1024**3\n",
    "    print(f\"VRAM after LoRA: {vram_used:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f8f6474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configuring training parameters...\n",
      "Training configuration:\n",
      "- Epochs: 3\n",
      "- Batch size per device: 2\n",
      "- Gradient accumulation: 4\n",
      "- Effective batch size: 8\n",
      "- Learning rate: 0.0002\n",
      "- Mixed precision: True\n"
     ]
    }
   ],
   "source": [
    "# Step 10: Configure Training Parameters\n",
    "# Set up training arguments for our fine-tuning process\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "print(\"\\nConfiguring training parameters...\")\n",
    "\n",
    "# Create output directory\n",
    "output_dir = \"./trained-models/fine-tuned-mistral-paintings\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Training arguments optimized for 12GB VRAM\n",
    "training_args = TrainingArguments(\n",
    "    # Output and logging\n",
    "    output_dir=output_dir,\n",
    "    run_name=\"mistral-paintings-audio-guide\",\n",
    "    \n",
    "    # Training schedule\n",
    "    num_train_epochs=3,                    # Number of complete passes through data\n",
    "    max_steps=-1,                          # -1 means use num_train_epochs instead\n",
    "    \n",
    "    # Batch sizes (adjust based on VRAM)\n",
    "    per_device_train_batch_size=2,         # Batch size per GPU for training\n",
    "    per_device_eval_batch_size=2,          # Batch size per GPU for evaluation\n",
    "    gradient_accumulation_steps=4,         # Effective batch size = 2 * 4 = 8\n",
    "    \n",
    "    # Learning rate and optimization\n",
    "    learning_rate=2e-4,                    # Learning rate for LoRA (higher than full fine-tuning)\n",
    "    weight_decay=0.01,                     # L2 regularization\n",
    "    warmup_steps=50,                       # Learning rate warmup\n",
    "    \n",
    "    # Evaluation and saving\n",
    "    eval_strategy=\"steps\",           # Evaluate every N steps\n",
    "    eval_steps=100,                        # Evaluate every 100 steps\n",
    "    save_strategy=\"steps\",                 # Save every N steps\n",
    "    save_steps=200,                        # Save every 200 steps\n",
    "    save_total_limit=3,                    # Keep only 3 checkpoints\n",
    "    \n",
    "    # Optimization settings\n",
    "    dataloader_drop_last=True,             # Drop incomplete batches\n",
    "    fp16=True,                             # Use mixed precision training\n",
    "    gradient_checkpointing=True,           # Save memory by recomputing activations\n",
    "    \n",
    "    # Logging\n",
    "    logging_steps=25,                      # Log every 25 steps\n",
    "    logging_dir=f\"{output_dir}/logs\",      # TensorBoard logs\n",
    "    report_to=None,                        # Disable wandb/tensorboard for now\n",
    "    \n",
    "    # Efficiency settings\n",
    "    load_best_model_at_end=True,          # Load best checkpoint at end\n",
    "    metric_for_best_model=\"eval_loss\",     # Use validation loss as metric\n",
    ")\n",
    "\n",
    "print(\"Training configuration:\")\n",
    "print(f\"- Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"- Batch size per device: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"- Gradient accumulation: {training_args.gradient_accumulation_steps}\")\n",
    "print(f\"- Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"- Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"- Mixed precision: {training_args.fp16}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dcacf7ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1829/2673414766.py:25: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Setting up data collator...\n",
      "Data collator configured for causal language modeling\n",
      "\n",
      "Initializing trainer...\n",
      " Trainer initialized successfully!\n",
      "\n",
      " Training Summary:\n",
      "- Model: mistralai/Mistral-7B-Instruct-v0.3 with LoRA\n",
      "- Training examples: 844\n",
      "- Validation examples: 94\n",
      "- Trainable parameters: 41,943,040\n",
      "- Estimated VRAM usage: ~7-8GB\n",
      "- Output directory: ./trained-models/fine-tuned-mistral-paintings\n",
      "\n",
      "Ready to start training!\n",
      "Next: Run trainer.train() to begin fine-tuning\n"
     ]
    }
   ],
   "source": [
    "# Step 11: Set up Data Collator\n",
    "# Prepare data batching for training\n",
    "\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "print(\"\\nSetting up data collator...\")\n",
    "\n",
    "# Data collator for causal language modeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # We're doing causal LM, not masked LM\n",
    "    pad_to_multiple_of=8,  # Pad to multiple of 8 for efficiency\n",
    ")\n",
    "\n",
    "print(\"Data collator configured for causal language modeling\")\n",
    "\n",
    "## Step 12: Initialize Trainer\n",
    "# Set up the training loop\n",
    "\n",
    "from transformers import Trainer\n",
    "\n",
    "print(\"\\nInitializing trainer...\")\n",
    "\n",
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer, \n",
    ")\n",
    "\n",
    "print(\" Trainer initialized successfully!\")\n",
    "\n",
    "# Print training summary\n",
    "print(f\"\\n Training Summary:\")\n",
    "print(f\"- Model: {MODEL_NAME} with LoRA\")\n",
    "print(f\"- Training examples: {len(tokenized_train)}\")\n",
    "print(f\"- Validation examples: {len(tokenized_val)}\")\n",
    "print(f\"- Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"- Estimated VRAM usage: ~7-8GB\")\n",
    "print(f\"- Output directory: {output_dir}\")\n",
    "\n",
    "print(\"\\nReady to start training!\")\n",
    "print(\"Next: Run trainer.train() to begin fine-tuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10638623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='318' max='318' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [318/318 42:12, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.358800</td>\n",
       "      <td>0.374975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.229600</td>\n",
       "      <td>0.303825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.124400</td>\n",
       "      <td>0.269072</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete model saved to ./trained-models/fine-tuned-mistral-paintings\n"
     ]
    }
   ],
   "source": [
    "# Step 13: Start Training\n",
    "\n",
    "print(\"Starting training...\")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "trainer.save_model()  \n",
    "model.save_pretrained(output_dir)  \n",
    "tokenizer.save_pretrained(output_dir) \n",
    "\n",
    "print(f\"Complete model saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8beaac89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the trained model (still in memory)...\n",
      "Generating audio guide...\n",
      "\n",
      "==================================================\n",
      "GENERATED AUDIO GUIDE:\n",
      "==================================================\n",
      "Discover Girl with a Pearl Earring, where Johannes Vermeer's refined technique shines through this 17.5 × 15.6 in (44.5 × 39.4 cm) Oil on canvas. Created in c. 1665, the color harmony demonstrates skill Notice the careful attention to detail reflecting the artistic values\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "## Test with the model still in memory from training\n",
    "\n",
    "print(\"Testing the trained model (still in memory)...\")\n",
    "\n",
    "# Use the model that was just trained (should still be in memory)\n",
    "# Make sure we're in eval mode\n",
    "model.eval()\n",
    "\n",
    "# Test prompt\n",
    "test_prompt = \"\"\"Title: Girl with a Pearl Earring\n",
    "Artist: Johannes Vermeer\n",
    "Date: c. 1665\n",
    "Medium: Oil on canvas\n",
    "Dimensions: 17.5 × 15.6 in (44.5 × 39.4 cm)\n",
    "\n",
    "Audio guide:\"\"\"\n",
    "\n",
    "# Tokenize input\n",
    "inputs = tokenizer(test_prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Move to same device as model\n",
    "if torch.cuda.is_available():\n",
    "    inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "\n",
    "# Generate\n",
    "print(\"Generating audio guide...\")\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs['input_ids'],\n",
    "        attention_mask=inputs['attention_mask'],\n",
    "        max_new_tokens=80,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "# Decode result\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "completion = generated_text[len(test_prompt):].strip()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"GENERATED AUDIO GUIDE:\")\n",
    "print(\"=\"*50)\n",
    "print(completion)\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d8c8be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model properly...\n",
      "Model saved to ./paintings-audio-guide-final\n"
     ]
    }
   ],
   "source": [
    "## Save model properly for future use\n",
    "\n",
    "print(\"Saving model properly...\")\n",
    "\n",
    "# Save the adapter only (recommended approach)\n",
    "model.save_pretrained(\"./trained-models/paintings-audio-guide-final\")\n",
    "\n",
    "# Save tokenizer\n",
    "tokenizer.save_pretrained(\"./trained-models/paintings-audio-guide-final\")\n",
    "\n",
    "print(\"Model saved to ./trained-models/paintings-audio-guide-final\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
